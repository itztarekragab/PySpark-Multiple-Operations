{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DIEH8iZqi-sk"
   },
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "oj1HhvIOY5Yz"
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as fun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SDp80mG9jmfU"
   },
   "source": [
    "### Building a Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ttzML9fpjE5a"
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('myApp').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TiqECDzLj1Mg"
   },
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Mx2qAccBk15y"
   },
   "outputs": [],
   "source": [
    "train = spark.read.csv('train.csv', header=True, inferSchema=True)\n",
    "test = spark.read.csv('test.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lvLJElPrlT4i"
   },
   "source": [
    "### Show 5 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "jYwhqvV8lnO0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7.25| null|       S|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|71.2833|  C85|       C|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|  7.925| null|       S|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|          113803|   53.1| C123|       S|\n",
      "|          5|       0|     3|Allen, Mr. Willia...|  male|35.0|    0|    0|          373450|   8.05| null|       S|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6QIYVxRXlnnw"
   },
   "source": [
    "### Display schema for the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "pcvERiICl1Ep"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PassengerId: integer (nullable = true)\n",
      " |-- Survived: integer (nullable = true)\n",
      " |-- Pclass: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- SibSp: integer (nullable = true)\n",
      " |-- Parch: integer (nullable = true)\n",
      " |-- Ticket: string (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Cabin: string (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xmE3Wd80l1S6"
   },
   "source": [
    "### Statistical summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "cNY0SItol5Mo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+-------------------+------------------+--------------------+------+------------------+------------------+-------------------+------------------+-----------------+-----+--------+\n",
      "|summary|      PassengerId|           Survived|            Pclass|                Name|   Sex|               Age|             SibSp|              Parch|            Ticket|             Fare|Cabin|Embarked|\n",
      "+-------+-----------------+-------------------+------------------+--------------------+------+------------------+------------------+-------------------+------------------+-----------------+-----+--------+\n",
      "|  count|              891|                891|               891|                 891|   891|               714|               891|                891|               891|              891|  204|     889|\n",
      "|   mean|            446.0| 0.3838383838383838| 2.308641975308642|                null|  null| 29.69911764705882|0.5230078563411896|0.38159371492704824|260318.54916792738| 32.2042079685746| null|    null|\n",
      "| stddev|257.3538420152301|0.48659245426485753|0.8360712409770491|                null|  null|14.526497332334035|1.1027434322934315| 0.8060572211299488|471609.26868834975|49.69342859718089| null|    null|\n",
      "|    min|                1|                  0|                 1|\"Andersson, Mr. A...|female|              0.42|                 0|                  0|            110152|              0.0|  A10|       C|\n",
      "|    25%|              223|                  0|                 2|                null|  null|              20.0|                 0|                  0|           19996.0|           7.8958| null|    null|\n",
      "|    50%|              446|                  0|                 3|                null|  null|              28.0|                 0|                  0|          236171.0|          14.4542| null|    null|\n",
      "|    75%|              669|                  1|                 3|                null|  null|              38.0|                 1|                  0|          347743.0|             31.0| null|    null|\n",
      "|    max|              891|                  1|                 3|van Melkebeke, Mr...|  male|              80.0|                 8|                  6|         WE/P 5735|         512.3292|    T|       S|\n",
      "+-------+-----------------+-------------------+------------------+--------------------+------+------------------+------------------+-------------------+------------------+-----------------+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.summary().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HiFaIEQTl70_"
   },
   "source": [
    "## EDA - Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PSNPOnP8mw2Q"
   },
   "source": [
    "### Display count for the train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "zrtpG11Fl9HM"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "891"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t_6nnTfxm9_x"
   },
   "source": [
    "### How many people survived, and how many didn't survive? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "QDoqPwyomYxA"
   },
   "outputs": [],
   "source": [
    "n_survived = train.where('Survived = 1').count()\n",
    "n_not_survived = train.where('Survived = 0').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P8DUtZXPn46m"
   },
   "source": [
    "### Display your result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "0XHAK8ceoCMU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "342 549\n"
     ]
    }
   ],
   "source": [
    "print(n_survived,n_not_survived)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ygsg7wQqor9a"
   },
   "source": [
    "### The Ratio of survival (using UDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "3uiaN29PoQnf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+\n",
      "|Survived|ratio(count)|\n",
      "+--------+------------+\n",
      "|1       |0.3838384   |\n",
      "|0       |0.61616164  |\n",
      "+--------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "total = train.count()\n",
    "grouped = train.groupby('Survived').count()\n",
    "\n",
    "@udf(returnType=FloatType()) \n",
    "def ratio(x):\n",
    "    return x/total\n",
    "\n",
    "grouped.select('Survived',ratio(fun.col('count'))).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q7Aker_lp1h4"
   },
   "source": [
    "### The number of males and females?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "XllkDlo3ongJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "577 314\n"
     ]
    }
   ],
   "source": [
    "n_males = train.where('Sex = \"male\"').count()\n",
    "n_females = train.where('Sex = \"female\"').count()\n",
    "print(n_males, n_females)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YHFaJ15zqtEV"
   },
   "source": [
    "### What is the average number of survivors of each gender?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "NUikH7MUqdKq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------+\n",
      "|   Sex|      avg(Survived)|\n",
      "+------+-------------------+\n",
      "|female| 0.7420382165605095|\n",
      "|  male|0.18890814558058924|\n",
      "+------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.select('Sex','Survived').groupBy(\"Sex\").avg(\"Survived\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YHFaJ15zqtEV"
   },
   "source": [
    "### What is the number of survivors of each gender?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|   Sex|count|\n",
      "+------+-----+\n",
      "|female|  233|\n",
      "|  male|  109|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.select('Sex','Survived').where('Survived = 1').groupBy(\"Sex\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kCEdYNdArtRN"
   },
   "source": [
    "### Creating a temporary view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "YjlK6HDUqsI5"
   },
   "outputs": [],
   "source": [
    "train.createOrReplaceTempView('myView')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JXNePifnshHr"
   },
   "source": [
    "### How many people survived, and how many didn't survive? (By SQL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "0HxfPRTMslqk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+\n",
      "|Survived|nSurvived|\n",
      "+--------+---------+\n",
      "|       1|      342|\n",
      "|       0|      549|\n",
      "+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT Survived, COUNT(Survived) AS nSurvived\n",
    "          FROM myView \n",
    "          GROUP BY Survived\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sVCdY6EasFWV"
   },
   "source": [
    "### The number of survivors from each gender as a ratio (By SQL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "7xQc3pUUr3HF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------+\n",
      "|   Sex|      survivalRatio|\n",
      "+------+-------------------+\n",
      "|female| 0.7420382165605095|\n",
      "|  male|0.18890814558058924|\n",
      "+------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT Sex,sum(Survived)/count(Survived) AS survivalRatio\n",
    "FROM myView\n",
    "GROUP BY Sex\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j6QXc5V8uu3Y"
   },
   "source": [
    "### Displaying a ratio for \"p-class\": SUM(Survived)/count for p-class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "Mscs2mDFdFsD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------+\n",
      "|Pclass|      survivalRatio|\n",
      "+------+-------------------+\n",
      "|     1| 0.6296296296296297|\n",
      "|     3|0.24236252545824846|\n",
      "|     2|0.47282608695652173|\n",
      "+------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT Pclass,sum(Survived)/count(Survived) AS survivalRatio\n",
    "FROM myView\n",
    "GROUP BY Pclass\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ctM9t8atxJl"
   },
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7CfanZTCt6Wk"
   },
   "source": [
    "### Merging both the train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "8Nm8S1K0r4uY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7.25| null|       S|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|71.2833|  C85|       C|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|  7.925| null|       S|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|          113803|   53.1| C123|       S|\n",
      "|          5|       0|     3|Allen, Mr. Willia...|  male|35.0|    0|    0|          373450|   8.05| null|       S|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_data = train.union(test)\n",
    "all_data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jI7AD8FLz3iO"
   },
   "source": [
    "### Display count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "s_WERAL8wvJa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1329"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5R4Miuy0z_uP"
   },
   "source": [
    "### The number of null values in each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "0LMOalKBxhpD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+----+---+---+-----+-----+------+----+-----+--------+\n",
      "|PassengerId|Survived|Pclass|Name|Sex|Age|SibSp|Parch|Ticket|Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+----+---+---+-----+-----+------+----+-----+--------+\n",
      "|          0|       0|     0|   0|  0|265|    0|    0|     0|   0| 1021|       3|\n",
      "+-----------+--------+------+----+---+---+-----+-----+------+----+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_data.select([fun.count(fun.when(fun.isnull(c),c)).alias(c) for c in all_data.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tBX8cJ000aqe"
   },
   "source": [
    "### Extracting only columns of null values alone and showing them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "ITmyUelNxjJM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+--------+\n",
      "|Age|Cabin|Embarked|\n",
      "+---+-----+--------+\n",
      "|265| 1021|       3|\n",
      "+---+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "null_data = all_data.select([fun.count(fun.when(fun.isnull(c),c)).alias(c) for c in all_data.columns])\n",
    "columns = []\n",
    "for c in null_data.columns :\n",
    "    if null_data.select(c).collect()[0][0]>0 :\n",
    "        columns.append(c)\n",
    "null_data = null_data.select(columns)\n",
    "null_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cuKrOi5a0-Ma"
   },
   "source": [
    "## Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lVQlr9vDy7Y4"
   },
   "source": [
    "### Creating a Temporary view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "xs3yeXhGI8rv"
   },
   "outputs": [],
   "source": [
    "train.createOrReplaceTempView('myView2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Txa8NZIO1JaP"
   },
   "source": [
    "### Showing the \"name\" column from your temporary table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "m7yXqJoJy35k"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                Name|\n",
      "+--------------------+\n",
      "|Braund, Mr. Owen ...|\n",
      "|Cumings, Mrs. Joh...|\n",
      "|Heikkinen, Miss. ...|\n",
      "|Futrelle, Mrs. Ja...|\n",
      "|Allen, Mr. Willia...|\n",
      "|    Moran, Mr. James|\n",
      "|McCarthy, Mr. Tim...|\n",
      "|Palsson, Master. ...|\n",
      "|Johnson, Mrs. Osc...|\n",
      "|Nasser, Mrs. Nich...|\n",
      "|Sandstrom, Miss. ...|\n",
      "|Bonnell, Miss. El...|\n",
      "|Saundercock, Mr. ...|\n",
      "|Andersson, Mr. An...|\n",
      "|Vestrom, Miss. Hu...|\n",
      "|Hewlett, Mrs. (Ma...|\n",
      "|Rice, Master. Eugene|\n",
      "|Williams, Mr. Cha...|\n",
      "|Vander Planke, Mr...|\n",
      "|Masselmani, Mrs. ...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT Name\n",
    "FROM myView2\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3F0F9cTZ2Cuz"
   },
   "source": [
    "### Using Regex for Pattern matching to create \"Title\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "0kx6OcB-2BBT"
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "all_data = all_data.withColumn('Title',F.regexp_extract(F.col(\"Name\"),\"([A-Za-z]+)\\.\",1))\n",
    "all_data.createOrReplaceTempView('combined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xbZeUWS12r59"
   },
   "source": [
    "### Displaying \"Title\" column and count \"Title\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "hGkFMtlp1FAI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+\n",
      "|   Title|counts|\n",
      "+--------+------+\n",
      "|     Mme|     1|\n",
      "|      Ms|     1|\n",
      "|     Don|     1|\n",
      "|    Capt|     2|\n",
      "|    Lady|     2|\n",
      "|     Sir|     2|\n",
      "|Jonkheer|     2|\n",
      "|Countess|     2|\n",
      "|   Major|     3|\n",
      "|     Col|     4|\n",
      "|    Mlle|     4|\n",
      "|     Rev|     9|\n",
      "|      Dr|    11|\n",
      "|  Master|    56|\n",
      "|     Mrs|   186|\n",
      "|    Miss|   257|\n",
      "|      Mr|   786|\n",
      "+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "titles = spark.sql(\"\"\"\n",
    "SELECT Title,count(Title) as counts\n",
    "FROM combined\n",
    "GROUP BY Title\n",
    "ORDER BY counts\n",
    "\"\"\")\n",
    "titles.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nLBQDKYu4JOa"
   },
   "source": [
    "#### We can see that Dr, Rev, Major, Col, Mlle, Capt, Don, Jonkheer, Countess, Ms, Sir, Lady, and Mme are really rare titles, so we'll create a Dictionary and set their values to \"rare\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "rjnx5l5r2Qaf"
   },
   "outputs": [],
   "source": [
    "titles_data = titles.collect()\n",
    "titles_map = {}\n",
    "for i in range(titles.count()):\n",
    "    if titles_data[i][1]<=11:\n",
    "        titles_map[titles_data[i][0]] = 'rare'\n",
    "    else:\n",
    "        titles_map[titles_data[i][0]] = titles_data[i][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Don': 'rare',\n",
       " 'Mme': 'rare',\n",
       " 'Ms': 'rare',\n",
       " 'Countess': 'rare',\n",
       " 'Lady': 'rare',\n",
       " 'Capt': 'rare',\n",
       " 'Sir': 'rare',\n",
       " 'Jonkheer': 'rare',\n",
       " 'Major': 'rare',\n",
       " 'Col': 'rare',\n",
       " 'Mlle': 'rare',\n",
       " 'Rev': 'rare',\n",
       " 'Dr': 'rare',\n",
       " 'Master': 'Master',\n",
       " 'Mrs': 'Mrs',\n",
       " 'Miss': 'Miss',\n",
       " 'Mr': 'Mr'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9wrE95Cv7Oqh"
   },
   "source": [
    "### Using a UDF to do the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "HdDbWuDl7Pf4"
   },
   "outputs": [],
   "source": [
    "@udf(returnType=StringType()) \n",
    "def impute_title(title):\n",
    "    return titles_map[title]# Title_map is your dictionary. please change this name with your dictionary name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f5EQVIhK7a9R"
   },
   "source": [
    "### Applying the function on \"Title\" column using UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "rBAiIOn77XFa"
   },
   "outputs": [],
   "source": [
    "all_data = all_data.withColumn(\"Title\", impute_title(F.col(\"Title\")))\n",
    "all_data.createOrReplaceTempView('myView3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sn8ewllf7kiV"
   },
   "source": [
    "### Displaying \"Title\" from table and group by \"Title\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "J9sjQb084GU6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "|count| Title|\n",
      "+-----+------+\n",
      "|   44|  rare|\n",
      "|   56|Master|\n",
      "|  186|   Mrs|\n",
      "|  257|  Miss|\n",
      "|  786|    Mr|\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT count(*) as count,Title\n",
    "             FROM myView3\n",
    "             GROUP BY Title\n",
    "             ORDER BY count\n",
    "          \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-H45QNLj9vJp"
   },
   "source": [
    "## **Preprocessing Age**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XwRAhumK-u__"
   },
   "source": [
    "### Based on the \"age\" column mean, we will fill in the missing age values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "eXYSVzvl4z63"
   },
   "outputs": [],
   "source": [
    "mean_age = all_data.select(fun.mean(fun.col('Age'))).collect()[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JLPivde8_GI-"
   },
   "source": [
    "### Fill missing with \"age\" mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "lBgW8aFD90PA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+------------------+-----+-----+----------------+-------+-----+--------+------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex|               Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked| Title|\n",
      "+-----------+--------+------+--------------------+------+------------------+-----+-----+----------------+-------+-----+--------+------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male|              22.0|    1|    0|       A/5 21171|   7.25| null|       S|    Mr|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female|              38.0|    1|    0|        PC 17599|71.2833|  C85|       C|   Mrs|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female|              26.0|    0|    0|STON/O2. 3101282|  7.925| null|       S|  Miss|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female|              35.0|    1|    0|          113803|   53.1| C123|       S|   Mrs|\n",
      "|          5|       0|     3|Allen, Mr. Willia...|  male|              35.0|    0|    0|          373450|   8.05| null|       S|    Mr|\n",
      "|          6|       0|     3|    Moran, Mr. James|  male|30.079501879699244|    0|    0|          330877| 8.4583| null|       Q|    Mr|\n",
      "|          7|       0|     1|McCarthy, Mr. Tim...|  male|              54.0|    0|    0|           17463|51.8625|  E46|       S|    Mr|\n",
      "|          8|       0|     3|Palsson, Master. ...|  male|               2.0|    3|    1|          349909| 21.075| null|       S|Master|\n",
      "|          9|       1|     3|Johnson, Mrs. Osc...|female|              27.0|    0|    2|          347742|11.1333| null|       S|   Mrs|\n",
      "|         10|       1|     2|Nasser, Mrs. Nich...|female|              14.0|    1|    0|          237736|30.0708| null|       C|   Mrs|\n",
      "|         11|       1|     3|Sandstrom, Miss. ...|female|               4.0|    1|    1|         PP 9549|   16.7|   G6|       S|  Miss|\n",
      "|         12|       1|     1|Bonnell, Miss. El...|female|              58.0|    0|    0|          113783|  26.55| C103|       S|  Miss|\n",
      "|         13|       0|     3|Saundercock, Mr. ...|  male|              20.0|    0|    0|       A/5. 2151|   8.05| null|       S|    Mr|\n",
      "|         14|       0|     3|Andersson, Mr. An...|  male|              39.0|    1|    5|          347082| 31.275| null|       S|    Mr|\n",
      "|         15|       0|     3|Vestrom, Miss. Hu...|female|              14.0|    0|    0|          350406| 7.8542| null|       S|  Miss|\n",
      "|         16|       1|     2|Hewlett, Mrs. (Ma...|female|              55.0|    0|    0|          248706|   16.0| null|       S|   Mrs|\n",
      "|         17|       0|     3|Rice, Master. Eugene|  male|               2.0|    4|    1|          382652| 29.125| null|       Q|Master|\n",
      "|         18|       1|     2|Williams, Mr. Cha...|  male|30.079501879699244|    0|    0|          244373|   13.0| null|       S|    Mr|\n",
      "|         19|       0|     3|Vander Planke, Mr...|female|              31.0|    1|    0|          345763|   18.0| null|       S|   Mrs|\n",
      "|         20|       1|     3|Masselmani, Mrs. ...|female|30.079501879699244|    0|    0|            2649|  7.225| null|       C|   Mrs|\n",
      "+-----------+--------+------+--------------------+------+------------------+-----+-----+----------------+-------+-----+--------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_data = all_data.na.fill(mean_age,subset=['Age'])\n",
    "all_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jGsnUz-m_P95"
   },
   "source": [
    "## **Preprocessing Embarked**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iHbbamcXMSYP"
   },
   "source": [
    "### Selecting \"Embarked\" column, counting them, ordering by count Desc, and saving in a variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "v-lRu5vc_FW7"
   },
   "outputs": [],
   "source": [
    "grouped_Embarked = spark.sql(\"\"\"SELECT count(*) as count,Embarked\n",
    "             FROM myView3\n",
    "             GROUP BY Embarked\n",
    "             ORDER BY Embarked Desc\n",
    "          \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E1qf5u2IOQrx"
   },
   "source": [
    "### Showing the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "jSFNDTNg_erb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+\n",
      "|count|Embarked|\n",
      "+-----+--------+\n",
      "|  962|       S|\n",
      "|  111|       Q|\n",
      "|  253|       C|\n",
      "|    3|    null|\n",
      "+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grouped_Embarked.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L8vhoEs8N2w_"
   },
   "source": [
    "### Filling missing values with max 'S' of grouped_Embarked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "LdzQCRud_mAa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+\n",
      "|count|Embarked|\n",
      "+-----+--------+\n",
      "|  965|       S|\n",
      "|  111|       Q|\n",
      "|  253|       C|\n",
      "+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_data=all_data.na.fill(value=\"S\",subset=[\"Embarked\"])\n",
    "all_data.createOrReplaceTempView('myView4')\n",
    "grouped_Embarked=spark.sql(\"\"\"SELECT count(*) as count,Embarked\n",
    "             FROM myView4\n",
    "             GROUP BY Embarked\n",
    "             ORDER BY Embarked Desc\n",
    "          \"\"\")\n",
    "grouped_Embarked.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TEcdV5Vb_qR_"
   },
   "source": [
    "## **Preprocessing Cabin**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_BQzPs7tqhpA"
   },
   "source": [
    "### Replacing \"cabin\" column with first char from the string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "4b6L5pK0_nQz"
   },
   "outputs": [],
   "source": [
    "first_letter = udf(lambda z: z[0] )\n",
    "all_data=all_data.withColumn(\"Cabin\", all_data.Cabin.substr(0, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6H8XshnYj4k2"
   },
   "source": [
    "### Showing the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "gJUQwnG1Oj2U"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+-----+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|Title|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+-----+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7.25| null|       S|   Mr|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|71.2833|    C|       C|  Mrs|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|  7.925| null|       S| Miss|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|          113803|   53.1|    C|       S|  Mrs|\n",
      "|          5|       0|     3|Allen, Mr. Willia...|  male|35.0|    0|    0|          373450|   8.05| null|       S|   Mr|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yzSDsWsUj9Im"
   },
   "source": [
    "### Creating the temporary view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "MR7CXTY7_tMJ"
   },
   "outputs": [],
   "source": [
    "all_data.createOrReplaceTempView('myView5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gv7lfQFkrLlN"
   },
   "source": [
    "### Counting each cabin and ordering them by cabin name descending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "A0tZG_mvrKXv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|count|Cabin|\n",
      "+-----+-----+\n",
      "|    1|    T|\n",
      "|    4|    G|\n",
      "|   18|    F|\n",
      "|   51|    E|\n",
      "|   52|    D|\n",
      "|   82|    C|\n",
      "|   77|    B|\n",
      "|   23|    A|\n",
      "| 1021| null|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT count(*) as count,Cabin\n",
    "             FROM myView5\n",
    "             GROUP BY Cabin\n",
    "             ORDER BY Cabin Desc\n",
    "          \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1GR6j0LOsB4y"
   },
   "source": [
    "### Filling missing values with \"U\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "mwq5CHEz_up_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|count|Cabin|\n",
      "+-----+-----+\n",
      "| 1021|    U|\n",
      "|    1|    T|\n",
      "|    4|    G|\n",
      "|   18|    F|\n",
      "|   51|    E|\n",
      "|   52|    D|\n",
      "|   82|    C|\n",
      "|   77|    B|\n",
      "|   23|    A|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_data = all_data.na.fill(value=\"U\",subset=[\"Cabin\"])\n",
    "all_data.createOrReplaceTempView('myView6')\n",
    "spark.sql(\"\"\"SELECT count(*) as count,Cabin\n",
    "             FROM myView6\n",
    "             GROUP BY Cabin\n",
    "             ORDER BY Cabin Desc\n",
    "          \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1DECL9yCK3JZ"
   },
   "source": [
    "### Using String indexer and OneHotEncoder for the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "odhuI2EHKuCm"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "\n",
    "categoricalCols = [field for (field, dataType) in all_data.dtypes\n",
    "                   if dataType == \"string\"]\n",
    "indexOutputCols = [x + \"_Index\" for x in categoricalCols]\n",
    "\n",
    "oheOutputCols = [x + \"_OHE\" for x in categoricalCols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "odhuI2EHKuCm"
   },
   "outputs": [],
   "source": [
    "stringIndexer = StringIndexer(inputCols=categoricalCols,\n",
    "                             outputCols=indexOutputCols,\n",
    "                             handleInvalid='skip')\n",
    "oheEncoder = OneHotEncoder(inputCols=indexOutputCols,\n",
    "                          outputCols=oheOutputCols)\n",
    "\n",
    "numericCols = [field for (field,dataType) in all_data.dtypes\n",
    "              if ((dataType!='string') & (field!='Survived') & (field!='crew'))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1FsiLsd9452v"
   },
   "source": [
    "### Using Vector Assembler to combine Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "BuytKk0hLE6p"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assemblerInputs = oheOutputCols + numericCols\n",
    "\n",
    "vecAssembler = VectorAssembler(inputCols=assemblerInputs,outputCol='features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dU8DeZfh7JIo"
   },
   "source": [
    "### Use randomSplit function and split data to trainDF, and testDF with 80% and 20% Consecutive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "8C11xf1iAzKp"
   },
   "outputs": [],
   "source": [
    "trainDF, testDF = all_data.randomSplit([.8,.2],seed=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IJQvmFai72O7"
   },
   "source": [
    "### Building a RandomForestClassifier model and using pipeline to fit and transform then display \"prediction, Survived, features\" columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "YnpmZqlTLPGq"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier \n",
    "\n",
    "randomforest = RandomForestClassifier(featuresCol='features',labelCol='Survived')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "pipeline = Pipeline(stages = [stringIndexer,oheEncoder,vecAssembler,randomforest])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+--------------------+\n",
      "|prediction|Survived|            features|\n",
      "+----------+--------+--------------------+\n",
      "|       0.0|       0|(1378,[629,761,12...|\n",
      "|       0.0|       1|(1378,[506,761,12...|\n",
      "|       1.0|       1|(1378,[528,1085,1...|\n",
      "|       0.0|       0|(1378,[445,761,10...|\n",
      "|       0.0|       0|(1378,[526,761,11...|\n",
      "+----------+--------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = pipeline.fit(trainDF).transform(testDF)\n",
    "\n",
    "predictions.select(\"prediction\", \"Survived\", \"features\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FSXEI8-r8bKY"
   },
   "source": [
    "### Using MulticlassClassificationEvaluator to get the log Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "Rl0UAKCaBDO-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log Loss is 0.536439913851112\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"Survived\", predictionCol=\"prediction\", metricName=\"logLoss\")\n",
    "print('log Loss is',evaluator.evaluate(predictions))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Practical_work.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
